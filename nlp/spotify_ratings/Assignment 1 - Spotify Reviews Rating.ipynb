{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySsIPBRiorxK"
   },
   "source": [
    "**Dataset**\n",
    "labeled datasset collected from Spotify (Assignment 1 - Spotify Reviews Rating)\n",
    "\n",
    "**Objective**\n",
    "classify Review to a category from 1 to 5. <br>\n",
    "\n",
    "**Total Estimated Time = 90-120 Mins**\n",
    "\n",
    "**Evaluation metric**\n",
    "macro f1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-6lrKz6orxT"
   },
   "source": [
    "### Import used libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "eXUPo3g4orxV"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "import re\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import make_scorer, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fG8MkuvjorxX"
   },
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BYeqhp66orxY"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time_submitted</th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7/9/2022 15:00</td>\n",
       "      <td>Great music service, the audio is high quality...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7/9/2022 14:21</td>\n",
       "      <td>Please ignore previous negative rating. This a...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7/9/2022 13:27</td>\n",
       "      <td>This pop-up \"Get the best Spotify experience o...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7/9/2022 13:26</td>\n",
       "      <td>Really buggy and terrible to use as of recently</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7/9/2022 13:20</td>\n",
       "      <td>Dear Spotify why do I get songs that I didn't ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time_submitted                                             Review  Rating\n",
       "0  7/9/2022 15:00  Great music service, the audio is high quality...       5\n",
       "1  7/9/2022 14:21  Please ignore previous negative rating. This a...       5\n",
       "2  7/9/2022 13:27  This pop-up \"Get the best Spotify experience o...       4\n",
       "3  7/9/2022 13:26    Really buggy and terrible to use as of recently       1\n",
       "4  7/9/2022 13:20  Dear Spotify why do I get songs that I didn't ...       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('Assignment 1 - Spotify Reviews Rating.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Time_submitted', 'Review', 'Rating'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good practice to split the data before EDA helps maintain the integrity of the machine learning process, prevents data leakage, simulates real-world scenarios more accurately, and ensures reliable model performance evaluation on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Time_submitted',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df['Review']\n",
    "y=df['Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49275,) (49275,) (12319,) (12319,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqWVKi_GorxZ"
   },
   "source": [
    "### EDA on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1zxJpFxorxa"
   },
   "source": [
    "- check NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HVEttSujorxa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.isna().sum() #no missing values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwjbzVaIorxb"
   },
   "source": [
    "- check duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "J_FlBWISorxb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.duplicated().sum() #there are duplications that need to be handled "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjIBFc35orxc"
   },
   "source": [
    "- show a representative sample of data texts to find out required preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zGFKzSCRorxc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love Spotify! I love how much music there is, and the mixes they give you, the podcasts and the end of year roundups\n",
      "*****************\n",
      "This new update is stupid. I can only listen like a few seconds of a song make the update back to the way it was\n",
      "*****************\n",
      "To many ads for non premium content üòí\n",
      "*****************\n",
      "Makes random beeping sounds unassociated with any activity. All notifications, sounds, .. are turned off. Spotify just randomly beeps ...very annoying. Im thinking of dropping it and going with another app.\n",
      "*****************\n",
      "I've been using this app for a couple of years. I have premium subscription and right now this app just logged me out without any reason. I can't log in! The official site is not responding. I live in Belarus and I'm really upset because in comparison with Russia we don't have podcasts and our library of music is smaller but app is much more expensive. Spotify canceled the premium subscription in Russia, but the free one still works. My friend has a free version and it also doesn't work\n",
      "*****************\n",
      "We have had spotify for 2 years now, and at first, it was great, but lately, I'm having to restart my phone, so this ap will play music. Like twice daily, I'm doing this; it doesn't matter if it is a podcast or music the same result. It's frustrating enough to consider going to Amazon....... Unless it's fixed, that's the way we will be taking. I\n",
      "*****************\n",
      "Great app for music and it's seamless multiple-device connectivity is amazing.\n",
      "*****************\n",
      "Really osam I like it I experianced and enjoy to explore the song\n",
      "*****************\n",
      "Android app is completely unusable with last update. Do yourself a favor and download a previous version and turn off auto update.\n",
      "*****************\n",
      "Premium is worth it. Simple and easy to use\n",
      "*****************\n"
     ]
    }
   ],
   "source": [
    "sample_texts = x_train.sample(n=10, random_state=3099)  \n",
    "for text in sample_texts:\n",
    "    print(text)\n",
    "    print(\"*****************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqdSUtbdorxd"
   },
   "source": [
    "- check dataset balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_distribution = y_train.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution:\n",
      "Rating\n",
      "5    17629\n",
      "1    14129\n",
      "4     6310\n",
      "2     5733\n",
      "3     5474\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Class Distribution:\")\n",
    "print(class_distribution) #data is not balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3wl_crkorxd"
   },
   "source": [
    "- Cleaning and Preprocessing are:\n",
    "    - 1 lowercasing\n",
    "    - 2 removing puctuation and special characters\n",
    "    - 3 Handling Emojis\n",
    "    - 4 Removing Stopwords\n",
    "    - 5 Lemmatization or Stemming\n",
    "    - 6 Expanding Contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyJkqK9gorxe"
   },
   "source": [
    "### Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "GVjzzLhworxe"
   },
   "outputs": [],
   "source": [
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,  lowercase=True,\n",
    "                 remove_punctuation=True, remove_stopwords=True, lemmatize=True):\n",
    "       \n",
    "        self.lowercase = lowercase\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatize = lemmatize\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        processed_texts = []\n",
    "        \n",
    "        for text in X:\n",
    "            \n",
    "            if self.lowercase:\n",
    "                text = text.lower()\n",
    "            \n",
    "            if self.remove_punctuation:\n",
    "                text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "            \n",
    "            if self.remove_stopwords:\n",
    "                tokens = word_tokenize(text)\n",
    "                filtered_tokens = [word for word in tokens if word not in self.stop_words]\n",
    "                text = ' '.join(filtered_tokens)\n",
    "            \n",
    "            \n",
    "            if self.lemmatize:\n",
    "                tokens = word_tokenize(text)\n",
    "                lemmatized_tokens = [self.lemmatizer.lemmatize(word) for word in tokens]\n",
    "                text = ' '.join(lemmatized_tokens)\n",
    "            \n",
    "            processed_texts.append(text)\n",
    "        \n",
    "        return processed_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ba0r1ASHorxf"
   },
   "source": [
    "**You  are doing Great so far!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9BhRQbYorxf"
   },
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### loading word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_fasttext_file = 'wiki-news-300d-1M.vec/wiki-news-300d-1M.vec' \n",
    "\n",
    "fasttext_model = KeyedVectors.load_word2vec_format(path_to_fasttext_file, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing fast text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vector for 'king': [ 1.082e-01  4.450e-02 -3.840e-02  1.100e-03 -8.880e-02  7.130e-02\n",
      " -6.960e-02 -4.770e-02  7.100e-03 -4.080e-02 -7.070e-02 -2.660e-02\n",
      "  5.000e-02 -8.240e-02  8.480e-02 -1.627e-01 -8.510e-02 -2.950e-02\n",
      "  1.534e-01 -1.828e-01 -2.208e-01  2.430e-02 -9.210e-02 -1.089e-01\n",
      " -1.009e-01 -1.190e-02  3.770e-02  2.038e-01  7.200e-02  2.020e-02\n",
      "  2.798e-01  1.150e-02 -1.510e-02  1.037e-01  4.000e-04 -1.040e-02\n",
      "  1.960e-02  1.265e-01  8.280e-02 -1.369e-01  1.070e-01  1.270e-01\n",
      " -3.490e-02 -6.830e-02 -1.140e-02  3.370e-02  1.260e-02  7.920e-02\n",
      "  4.400e-02 -2.530e-02  4.890e-02 -7.850e-02 -6.259e-01 -9.720e-02\n",
      "  1.654e-01 -5.780e-02 -4.370e-02  4.090e-02 -1.820e-02 -1.891e-01\n",
      "  2.770e-02 -1.460e-02 -5.310e-02  4.260e-02  4.900e-03  4.000e-03\n",
      "  1.423e-01 -9.750e-02 -3.500e-03  9.630e-02 -1.900e-03 -1.466e-01\n",
      " -1.662e-01  6.650e-02 -1.500e-01 -1.267e-01  2.670e-02 -1.560e-01\n",
      " -1.442e-01  1.515e-01  2.420e-02 -6.080e-02  9.180e-02 -2.407e-01\n",
      " -4.110e-02 -1.420e-02  6.550e-02 -3.590e-02  1.459e-01  9.400e-02\n",
      "  1.590e-02  6.380e-02 -1.077e-01 -5.170e-02 -1.370e-02  5.120e-02\n",
      " -2.750e-02 -5.070e-02  6.900e-03  3.660e-02 -1.529e-01 -1.813e-01\n",
      "  3.390e-02 -8.510e-02 -5.400e-02  1.180e-01  1.039e-01  6.190e-02\n",
      " -2.350e-02 -1.150e-02  1.648e-01  9.360e-02 -5.000e-03 -9.790e-02\n",
      " -5.890e-02 -7.210e-02 -1.586e-01  2.270e-02 -4.460e-02 -3.398e-01\n",
      " -2.840e-02 -2.507e-01  4.510e-02 -1.226e-01  8.000e-02  2.365e-01\n",
      "  7.560e-02 -8.530e-02  1.157e-01  2.780e-02  7.100e-02 -1.314e-01\n",
      " -4.630e-02  4.270e-02 -5.050e-02 -2.490e-02  1.182e-01  4.810e-02\n",
      " -1.085e-01 -1.600e-02  3.900e-03 -3.860e-02  1.551e-01  2.695e-01\n",
      "  7.070e-02 -8.420e-02  1.167e-01  8.450e-02 -1.040e-02  2.060e-02\n",
      "  4.690e-02  5.700e-03  8.970e-02  7.230e-02  2.220e-02  7.270e-02\n",
      "  6.420e-02 -2.350e-02 -2.160e-02 -6.010e-02  5.370e-02 -2.842e-01\n",
      " -1.047e-01  1.733e-01  2.100e-03 -1.050e-02  1.143e-01  2.150e-02\n",
      "  7.400e-03 -5.040e-02 -4.900e-03  1.190e-02 -2.700e-02  1.450e-02\n",
      "  9.670e-02  9.030e-02  3.145e-01  1.222e-01  9.850e-02  2.126e-01\n",
      " -1.030e-01  7.930e-02 -7.870e-02 -5.930e-02  7.390e-02 -6.960e-02\n",
      " -8.180e-02  3.200e-02 -1.808e-01  4.770e-02  8.250e-02 -1.270e-02\n",
      "  1.445e-01 -6.050e-02 -5.130e-02  9.450e-02 -1.030e-01  4.750e-02\n",
      "  9.820e-02  2.402e-01  8.600e-03 -2.410e-02 -3.320e-02  4.300e-02\n",
      " -4.170e-02  1.990e-02 -5.280e-02 -6.300e-02  3.470e-02  5.800e-02\n",
      " -2.600e-02  1.113e-01  9.890e-02 -3.800e-03 -1.272e-01 -9.790e-02\n",
      "  4.500e-03  6.100e-03 -3.980e-02 -8.500e-03 -3.500e-03 -1.191e-01\n",
      " -9.490e-02  1.230e-02  1.705e-01 -2.065e-01  5.500e-02  4.530e-02\n",
      "  4.240e-02 -5.780e-02 -3.480e-02 -1.770e-02  3.437e-01 -6.590e-02\n",
      "  9.240e-02 -1.122e-01 -1.588e-01  1.068e-01 -3.029e-01  1.800e-03\n",
      "  3.170e-02  1.857e-01  3.600e-02  8.290e-02  2.240e-02  9.340e-02\n",
      " -4.750e-02  1.719e-01  1.500e-03  4.849e-01 -2.280e-02 -9.020e-02\n",
      "  4.650e-02 -1.087e-01  1.374e-01  1.150e-02 -1.246e-01  5.090e-02\n",
      "  1.578e-01 -1.667e-01 -3.400e-02  4.690e-02  5.680e-02  1.599e-01\n",
      " -3.915e-01  3.560e-02  2.870e-02 -2.275e-01 -1.378e-01 -2.650e-02\n",
      " -1.115e-01  1.804e-01  7.960e-02 -9.870e-02  9.050e-02  3.556e-01\n",
      "  2.400e-02  2.460e-02  2.830e-02  6.090e-02 -2.270e-02 -4.690e-02\n",
      " -5.350e-02  4.400e-02  1.021e-01 -1.398e-01  5.370e-02 -2.549e-01\n",
      "  8.270e-02 -1.011e-01  4.700e-03 -7.120e-02  1.442e-01 -7.000e-02\n",
      "  1.230e-02  3.440e-02 -5.700e-02  1.580e-02  5.440e-02  2.560e-02]\n"
     ]
    }
   ],
   "source": [
    "word_vector = fasttext_model['king']\n",
    "print(\"Word vector for 'king':\", word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to 'king': [('kings', 0.7969563603401184), ('queen', 0.7638539671897888), ('monarch', 0.739997148513794), ('King', 0.7281951904296875), ('prince', 0.7132730484008789)]\n"
     ]
    }
   ],
   "source": [
    "similar_words = fasttext_model.most_similar('king', topn=5)\n",
    "print(\"Most similar words to 'king':\", similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_embeddings(text, embedding_model, embedding_size=300):\n",
    "    words = text.split()\n",
    "    embeddings = []\n",
    "    for word in words:\n",
    "        if word in embedding_model:\n",
    "            embeddings.append(embedding_model[word])\n",
    "        else:\n",
    "            embeddings.append([0] * embedding_size) \n",
    "\n",
    "    return embeddings\n",
    "\n",
    "X_train_embeddings = [text_to_embeddings(text, fasttext_model) for text in x_train]\n",
    "X_test_embeddings = [text_to_embeddings(text, fasttext_model) for text in x_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\iti\\python\\conda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1228/1228\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - f1_score: 1.5170 - loss: -49343.8672 - val_f1_score: 1.5216 - val_loss: -794491.7500\n",
      "Epoch 2/10\n",
      "\u001b[1m1228/1228\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - f1_score: 1.5176 - loss: -1565785.0000 - val_f1_score: 1.5216 - val_loss: -5232335.5000\n",
      "Epoch 3/10\n",
      "\u001b[1m1228/1228\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - f1_score: 1.5170 - loss: -7098204.0000 - val_f1_score: 1.5216 - val_loss: -14742295.0000\n",
      "Epoch 4/10\n",
      "\u001b[1m1228/1228\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - f1_score: 1.5178 - loss: -17889386.0000 - val_f1_score: 1.5216 - val_loss: -30048072.0000\n",
      "Epoch 5/10\n",
      "\u001b[1m1228/1228\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - f1_score: 1.5181 - loss: -34525836.0000 - val_f1_score: 1.5216 - val_loss: -51875628.0000\n",
      "Epoch 6/10\n",
      "\u001b[1m1228/1228\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - f1_score: 1.5189 - loss: -58076004.0000 - val_f1_score: 1.5216 - val_loss: -80930704.0000\n",
      "Epoch 7/10\n",
      "\u001b[1m1228/1228\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - f1_score: 1.5167 - loss: -88051048.0000 - val_f1_score: 1.5216 - val_loss: -118050856.0000\n",
      "Epoch 8/10\n",
      "\u001b[1m1228/1228\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - f1_score: 1.5166 - loss: -126501952.0000 - val_f1_score: 1.5216 - val_loss: -163890272.0000\n",
      "Epoch 9/10\n",
      "\u001b[1m1228/1228\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - f1_score: 1.5172 - loss: -174108800.0000 - val_f1_score: 1.5216 - val_loss: -219176272.0000\n",
      "Epoch 10/10\n",
      "\u001b[1m1228/1228\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - f1_score: 1.5173 - loss: -231111408.0000 - val_f1_score: 1.5216 - val_loss: -284801696.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2bc41c9ce90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train_flattened = np.array([np.mean(embeddings, axis=0) for embeddings in X_train_embeddings])\n",
    "X_test_flattened = np.array([np.mean(embeddings, axis=0) for embeddings in X_test_embeddings])\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(300,)), \n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid') \n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['f1_score'])\n",
    "model.fit(X_train_flattened, y_train, epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85JlkIQXorxg"
   },
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation metric:**\n",
    "macro f1 score\n",
    "\n",
    "Macro F1 score is a useful metric in scenarios where you want to evaluate the overall performance of a multi-class classification model, **particularly when the classes are imbalanced**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Calculation](https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/639c3d934e82c1195cdf3c60_macro-f1.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m385/385\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 638us/step - f1_score: 1.5203 - loss: -287219872.0000\n",
      "Test Accuracy: <function f1_score at 0x000002BBC37991C0>\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test data\n",
    "loss, accuracy = model.evaluate(X_test_flattened, y_test)\n",
    "print(f\"Test Accuracy: {f1_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhVFUaIcorxh"
   },
   "source": [
    "### Enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDsCjdqsorxi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion and final results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nw1GVnYLorxi"
   },
   "source": [
    "#### Done!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
